\section{Parameter-efficient fine-tuning}

As we observed in question 1, fine-tuning the entire model can become extremely costly for very large models. In this question, we'll explore more methods for \textit{parameter-efficient fine-tuning}, i.e., methods that enable fine-tuning while creating fewer new parameters and are less prone to overfitting.

\begin{enumerate}[label={3.\alph*}]
    \input{03-parameter-efficient-ft/01-implement}
    \input{03-parameter-efficient-ft/02-LoRA}
    \input{03-parameter-efficient-ft/03-implement-LoRA}
    \input{03-parameter-efficient-ft/04-evaluate}
\end{enumerate}