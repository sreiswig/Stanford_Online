\item \points{3b} {\bf LoRA}

In addition to selecting only a subset of layers to fine-tune, more sophisticated methods for parameter-efficient fine-tuning exist; one such method is \href{https://arxiv.org/pdf/2106.09685.pdf}{LoRA: Low-rank adaptation}. For each layer $\ell$ in the network, rather than fine-tune the pre-trained weight matrix $W_\ell^0 \in \mathbb{R}^{d_1\times d_2}$ into an arbitrary new weight matrix $W_\ell^{ft}$, LoRA constrains the space of fine-tuned parameters such that $W_\ell^{ft} = W_\ell^0 + AB^\top$, where $A \in \mathbb{R}^{d_1\times p}$ and $B \in \mathbb{R}^{d_2\times p}$, and $p << d_1,d_2$. That is, we force the \textit{difference} between $W_\ell^0$ and $W_\ell^{ft}$ to be rank $p$, keeping $W_\ell^0$ frozen and only fine-tuning the rank-$p$ residual matrix $AB^\top$. We will apply this form of fine-tuning to both the MLP weight matrices and the self-attention weight matrices in the model.

For a single layer, what are the parameter savings we achieve by using LoRA? i.e., what is the ratio of parameters fine-tuned by LoRA (for arbitrary $p$) to the number of parameters in $W_\ell^0$? In terms of $p,d_1,d_2$, when will LoRA provide the greatest savings in newly-created parameters?
